# -*- coding: utf-8 -*-

"""Main module."""
from __future__ import print_function
import os
import uuid
import tarfile
import time
from pprint import pprint
import shutil
import boto3
from botocore.client import Config
import kubernetes.client
import kubernetes.config
from kubernetes.client.rest import ApiException
import pandas as pd
import tempfile

#Global definition strings
group = 'argoproj.io' # str | The custom resource's group name
version = 'v1alpha1' # str | The custom resource's version
namespace = 'argo' # str | The custom resource's namespace
plural = 'workflows' # str | The custom resource's plural name. For TPRs this would be lowercase plural kind.

AWS_HOST = os.getenv('AWS_HOST')
AWS_ENDPOINT = os.getenv('AWS_ENDPOINT')
AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')


def setup_k8s_api():
    """
    Common actions to setup Kubernetes API access to Argo workflows
    """
    kubernetes.config.load_incluster_config() #Only works inside of JupyterLab Pod
    configuration = kubernetes.client.Configuration() #Create configuration
    
    return kubernetes.client.CustomObjectsApi(kubernetes.client.ApiClient(configuration)) 

def list_argo_jobs():
    """
    This will return a list of all jobs associated with the current user
    """
    api_instance = setup_k8s_api()
    
    try:
        api_response = api_instance.list_namespaced_custom_object(group, version, namespace, plural)
        data = []
        for item in api_response['items']:
            notebook_name = ''
            if 'notebook-name' in item['metadata']['labels']:
                notebook_name = f"{item['metadata']['labels']['notebook-name']}"
            data.append([f"{item['metadata']['name']}", f"{item['status']['phase']}", notebook_name, f"{item['status']['startedAt']}"])
        df = pd.DataFrame(data, columns=["NAME", "STATUS", "NOTEBOOK NAME", "STARTED AT"])
        df['STARTED AT'] = pd.to_datetime(df['STARTED AT'])
        df.set_index('STARTED AT', inplace=True)
        df = df.sort_values(by=['STARTED AT'], ascending=False)
        pprint(df)
    except ApiException as e:
        print("Exception when calling CustomObjectsApi->list_namespaced_custom_object: %s\n" % e)
    return
    
def create_argo_job(body):
    api_instance = setup_k8s_api()
            
    try: 
        api_response = api_instance.create_namespaced_custom_object(group, version, namespace, plural, body)
        pprint(api_response)
    except ApiException as e:
        print("Exception when calling CustomObjectsApi->create_namespaced_custom_object: %s\n" % e)
                         
def schedule_notebook(filename):
    """
    Send the Notebook with the given filename to Argo cluster for execution
    """
                         
    job_uuid = uuid.uuid4().hex
                         
    # Put Notebook file into local temporary archive
    local_temp_archive_handle, local_temp_archive_path = tempfile.mkstemp(suffix='.tgz')
    os.close(local_temp_archive_handle)
    tar = tarfile.open(local_temp_archive_path, "w:gz")
    tar.add(filename)
    tar.close()
    
    # Upload archive to S3 bucket
    #Setup S3 resource
    try:
        assert(AWS_HOST!=None)
        assert(AWS_ENDPOINT!=None)
        assert(AWS_ACCESS_KEY_ID!=None)
        assert(AWS_SECRET_ACCESS_KEY!=None)
    except AssertionError:
        print("Error: S3 credentials are not set up")
    
    s3 = boto3.resource('s3',
                        endpoint_url=AWS_ENDPOINT,
                        aws_access_key_id=AWS_ACCESS_KEY_ID,
                        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
                        config=Config(signature_version='s3v4'))

    #Create bucket object
    bucket = s3.Bucket('rookbucket')
                         
    #Upload archive to S3
    hostname = os.environ.get('HOSTNAME') #should have a hostname structure 'jupyter-<username>'
    assert(hostname!=None) #check that env variable was read correctly
    username = hostname[8:] #Remove prefix 'jupyter-' from hostmane to get username
    
    # Archived notebook is stored in S3 bucket at the following path:
    # '/<username>/inputs/<workflow_name>.tgz'
    # <workflow_name> is unique random string generated by uuid
    s3_archive_uuid = job_uuid
    s3_archive_key = username + "/inputs/" + s3_archive_uuid + ".tgz"
    bucket.upload_file(local_temp_archive_path, s3_archive_key)
    
    #Check that upload was successful and delete local archive
    try:
        [bucket_object.key for bucket_object in bucket.objects.all()].index(s3_archive_key)
    except ValueError:
        print('Error: Notebook upload failed')
    os.remove(local_temp_archive_path)
            
    # Define the JSON schema of the Workflow Custom Resource Definition to create.
    body = {
      "apiVersion": "argoproj.io/v1alpha1",
      "kind": "Workflow",
      "metadata": {
        "name": username + "-workflow-" + job_uuid,
        "namespace": "argo",
        "labels": {
            "notebook-name": filename
        }
      },
      "spec": {
        "serviceAccountName": "workflow",
        "arguments": {
          "artifacts": [
            {
              "name": "notebook-in",
              "s3": {
                "accessKeySecret": {
                  "key": "AccessKey",
                  "name": "rook-ceph-object-user-my-store-my-user"
                },
                "bucket": "rookbucket",
                "endpoint": "rook-ceph-rgw-my-store.rook-ceph",
                "insecure": True,
                "key": s3_archive_key,
                "secretKeySecret": {
                  "key": "SecretKey",
                  "name": "rook-ceph-object-user-my-store-my-user"
                }
              }
            }
          ]
        },
        "entrypoint": "papermill",
        "templates": [
          {
            "container": {
              "command": [
                "sh",
                "-c",
                "papermill /tmp/in.ipynb /tmp/out.ipynb"
              ],
              "image": "ktaletsk/polyglot-jupyter:latest"
            },
            "inputs": {
              "artifacts": [
                {
                  "name": "notebook-in",
                  "path": "/tmp/in.ipynb"
                }
              ]
            },
            "name": "papermill",
            "outputs": {
              "artifacts": [
                {
                  "name": "notebook-out",
                  "path": "/tmp/out.ipynb"
                }
              ]
            }
          }
        ]
      }
    }    
    create_argo_job(body)